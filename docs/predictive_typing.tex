\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template
\usepackage{amsmath}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyfoot[RO,LE]{\thepage} % Custom footer text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Predictive Typing System \\ \fontsize{18pt}{10pt}\selectfont Text Analysis and Retrieval}} % Predictive typing

\author{
\large
\textsc{Matija Šantl}\\\textsc{Mihael Šafarić}\\
\normalsize Faculty of Electrical Engineering and Computing \\
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}
In this paper we survey two algorithms for smoothing models for language n-gram modeling, Witten Bell and Kneser Ney. We implemented simple text editor which uses this methods and then we investigate how factors such as training data size, training corpus, and n-gram order affect to the finally results of this methods which is measured through TODO. We find that these factors can significantly affect the performance of models, especially training data size. TODO Results are showing ...
\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2}

\section{Introduction}
A language model is a probability distribution p(s) over string s that describes how often the string s occurs as a sentence in some domain of interest. 
Language models provide text entry system with the power to predict unseen text likely to be generated by a user and they are used in many tasks including speech recognition, optical character recognition, handwriting recognition, machine translation, and spelling correction.  That enables people to create text in collaboration with machines which improves efficiency and makes usage of electronic devices more accessible to disabled people. The most commonly used language models are trigram models and they determine the probability of a word given the previous two words, $ p(\omega_i|\omega_{i-2}\omega_{i-1}) $. The simplest way to approximate this probability is to compute 
\begin{equation}
p(\omega_i|\omega_{i-2}\omega_{i-1}) = \frac{c(\omega_{i-2}\omega_{i-1}\omega_i)}{c(\omega_{i-2}\omega_{i-1})}
\end{equation}
where the $ c(\omega_{i-2}\omega_{i-1}\omega_i) $ is the number of times the word sequence $ \omega_{i-2}\omega_{i-1}\omega_i $ occurs in some corpus of training data and $ c(\omega_{i-2}\omega_{i-1}) $ is the number of times word sequence $ \omega_{i-2}\omega_{i-1} $ occurs. This probability is called the maximum likelihood estimate. Maximum likelihood estimate can lead to poor performance in many applications of language models, for example, if the correct ... 

\section{Methods}


\section{Results}

\section{Discussion}

\section{Conclusion}
TODO U 2... u Introduction ima zakaj je predictin typing dobar, a zakaj ne

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliography{bibliography}{}
\nocite{*}
\bibliographystyle{abbrv}

%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
